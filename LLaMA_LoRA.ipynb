{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zetavg/LLaMA-LoRA-Tuner/blob/main/LLaMA_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bb4nzBvLfZUj"
      },
      "source": [
        "# 🌾🎛️ Khao-LoRA Tuner\n",
        "\n",
        "TL;DR: **Runtime > Run All** (`⌘/Ctrl+F9`). Takes about 5 minutes to start. You will be promped to authorize Google Drive access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DwarOgXbG77C"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<audio src=\"https://github.com/anars/blank-audio/raw/master/1-hour-of-silence.mp3\" autoplay muted loop controls />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcJ4WO3KhOX1"
      },
      "outputs": [],
      "source": [
        "%pip install Pillow==9.3.0 numpy==1.23.5\n",
        "\n",
        "import pkg_resources as r\n",
        "import PIL\n",
        "import numpy\n",
        "for module, min_version in [(PIL, \"9.3\"), (numpy, \"1.23\")]:\n",
        "  lib_version = r.parse_version(module.__version__)\n",
        "  print(module.__name__, lib_version)\n",
        "  if lib_version < r.parse_version(min_version):\n",
        "    raise Exception(\"Restart the runtime by clicking the 'RESTART RUNTIME' button above (or Runtime > Restart Runtime).\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5uS5jJ8063f_"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3ZCPW0JBCcH"
      },
      "outputs": [],
      "source": [
        "llama_lora_project_url = \"https://github.com/krittapat-canik/Khao-tuner.git\" # @param {type:\"string\"}\n",
        "llama_lora_project_branch = \"main\" # @param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZmRtUY68U5f"
      },
      "outputs": [],
      "source": [
        "google_drive_folder = \"Colab Data/LLaMA-LoRA Tuner\" # @param {type:\"string\"}\n",
        "google_drive_mount_path = \"/content/drive\"\n",
        "\n",
        "from requests import get\n",
        "from socket import gethostname, gethostbyname\n",
        "host_ip = gethostbyname(gethostname())\n",
        "colab_notebook_filename = get(f\"http://{host_ip}:9000/api/sessions\").json()[0][\"name\"]\n",
        "from google.colab import drive\n",
        "try:\n",
        "  drive.mount(google_drive_mount_path)\n",
        "\n",
        "  google_drive_data_directory_relative_path = google_drive_folder\n",
        "  google_drive_data_directory_path = f\"{google_drive_mount_path}/My Drive/{google_drive_data_directory_relative_path}\"\n",
        "  !mkdir -p \"{google_drive_data_directory_path}\"\n",
        "  !ln -nsf \"{google_drive_data_directory_path}\" ./data\n",
        "  !touch \"data/This folder is used by the Colab notebook \\\"{colab_notebook_filename}\\\".txt\"\n",
        "  !echo \"Data will be stored in Google Drive folder: \\\"{google_drive_data_directory_relative_path}\\\", which is mounted under \\\"{google_drive_data_directory_path}\\\"\"\n",
        "except Exception as e:\n",
        "  print(\"Drive won't be mounted!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ep3Qhwj0Bzwf"
      },
      "outputs": [],
      "source": [
        "base_model = \"decapoda-research/llama-7b-hf\" # @param {type:\"string\"}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg8tzrgb6ya_"
      },
      "source": [
        "# Runtime Info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHbRt8sK6YWy"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGM5MYjR7yeS"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8vSPMNtNAqOo"
      },
      "source": [
        "# Prepare the Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGYz2VDoAzC8"
      },
      "outputs": [],
      "source": [
        "![ ! -d llama_lora ] && git clone -b {llama_lora_project_branch} --filter=tree:0 {llama_lora_project_url} llama_lora\n",
        "!cd llama_lora && git add --all && git stash && git fetch origin {llama_lora_project_branch} && git checkout {llama_lora_project_branch} && git reset origin/{llama_lora_project_branch} --hard\n",
        "![ ! -f llama-lora-requirements-installed ] && cd llama_lora && pip install -r requirements.lock.txt && touch ../llama-lora-requirements-installed"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "o90p1eYQimyr"
      },
      "source": [
        "# Launch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYVjcvwXimB6"
      },
      "outputs": [],
      "source": [
        "# The following command will launch the app in one shot, but we will not do this here.\n",
        "# Instead, we will import and run Python code from the runtime, so that each part\n",
        "# can be reloaded easily in the Colab notebook and provide readable outputs.\n",
        "# It also resolves the GPU out-of-memory issue on training.\n",
        "# !python llama_lora/app.py --base_model='{base_model}' --data_dir='./data' --share"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf6g248ylteP"
      },
      "outputs": [],
      "source": [
        "# Set Configs\n",
        "from llama_lora.llama_lora.config import Config, process_config\n",
        "from llama_lora.llama_lora.globals import initialize_global\n",
        "Config.default_base_model_name = base_model\n",
        "Config.base_model_choices = [base_model]\n",
        "data_dir_realpath = !realpath ./data\n",
        "Config.data_dir = data_dir_realpath[0]\n",
        "Config.load_8bit = True\n",
        "process_config()\n",
        "initialize_global()\n",
        "\n",
        "# Prepare Data Dir\n",
        "from llama_lora.llama_lora.utils.data import init_data_dir\n",
        "init_data_dir()\n",
        "\n",
        "# Load the Base Model\n",
        "from llama_lora.llama_lora.models import prepare_base_model\n",
        "prepare_base_model()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K-hCouBClKAe"
      },
      "source": [
        "## Start Gradio UI 🚀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLygNTcHk0N8"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from llama_lora.llama_lora.ui.main_page import main_page, get_page_title\n",
        "from llama_lora.llama_lora.ui.css_styles import get_css_styles\n",
        "\n",
        "with gr.Blocks(title=get_page_title(), css=get_css_styles()) as app:\n",
        "    main_page()\n",
        "\n",
        "app.queue(concurrency_count=1).launch(share=True, debug=True, server_name=\"127.0.0.1\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
